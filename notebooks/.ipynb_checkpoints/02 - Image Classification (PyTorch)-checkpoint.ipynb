{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "\"Deep Learning\" is a general term that usually refers to the use of neural networks with multiple layers that synthesize the way the human brain learns and makes decisions. A convolutional neural network is a kind of neural network that extracts *features* from matrices of numeric values (often images) by convolving multiple filters over the matrix values to apply weights and identify patterns, such as edges, corners, and so on in an image. The numeric representations of these patterns are then passed to a fully-connected neural network layer to map the features to specific classes.\n",
    "\n",
    "## Basic Neural Network Recap\n",
    "\n",
    "Your brain works by connecting networks of neurons, each of which receives electrochemical stimuli from multiple inputs, which cause the neuron to fire under certain conditions. When a neuron fires, it creates an electrochemical charge that is passed as an input to one or more other neurons, creating a complex *feed-forward* network made up of layers of neurons that pass the signal on. An artificial neural network uses the same principles but the inputs are numeric values with associated *weights* that reflect their relative importance. The neuron take these input values and weights and applies them to an *activation function* that determines the ouput that the artificial neuron passes onto the next layer:\n",
    "\n",
    "<br/>\n",
    "<div align=\"center\" style='font-size:24px;'>&#8694;&#9711;&rarr;</div>\n",
    "\n",
    "As the human brain learns from experience, the inputs to the neurons are strenghtened or weakened depending on their importance to the decisions that the brain needs to make in response to stimuli. Similarly, you train an artificial neural network using a supervised leaning technique in which a *loss function* is used to evaluate how well the multi-layered model detects known labels. You can then find the derivative of the loss function to determine whether the level of error (loss) is reduced by increasing or decreasing the weights associated with the inputs, and then apply *backpropagation* to adjust the weights and improve the model iteratively over multiple training *epochs*. The result of this training is a deep learning model that consists of:\n",
    "* An *input* layer to which the initial input variables are passed.\n",
    "* One or more *hidden* layers in which the weights optimized by training determine the signal that is fed forward through the network.\n",
    "* An *output* layer that presents the results.\n",
    "\n",
    "## Convolutional Neural Networks (CNNs)\n",
    "Convolutional Neural Networks, or *CNNs*, are a particular type of artificial neural network that works well with matrix inputs, such as images (which are fundamentally just multi-dimensional matrices of pixel intensity values). There are various kinds of layer in a CNN, but a common architecture is to build a sequence of *convolutional* layers that find patterns in indvidual areas of the input matrix and *pooling* layers that aggregate these patterns. Additionally, some layers may *drop* data (which helps avoid *overfitting* the model to the training data), and finally some layers will *flatten* the matrix data and a linear *dense*, or *fully connected* layer will perform classification and reshape the predictions to conform with the expected output format.\n",
    "\n",
    "Conceptually, a Convolutional Neural Network for image classification is made up of multiple layers that extract features, such as edges, corners, etc; followed by one or more fully-connected layers to classify objects based on these features. You can visualize this like this:\n",
    "\n",
    "<table>\n",
    "    <tr><td rowspan=2 style='border: 1px solid black;'>&#x21d2;</td><td style='border: 1px solid black;'>Convolutional Layer</td><td style='border: 1px solid black;'>Pooling Layer</td><td style='border: 1px solid black;'>Convolutional Layer</td><td style='border: 1px solid black;'>Pooling Layer</td><td style='border: 1px solid black;'>Drop Layer</td><td style='border: 1px solid black;'>Fully Connected Layer</td><td rowspan=2 style='border: 1px solid black;'>&#x21d2;</td></tr>\n",
    "    <tr><td colspan=5 style='border: 1px solid black; text-align:center;'>Feature Extraction</td><td style='border: 1px solid black; text-align:center;'>Classification</td></tr>\n",
    "</table>\n",
    "\n",
    "*Note: In Machine Learning, particularly \"deep learning\", matrices used in neural networks are often referred to as **tensors**. In a simplistic (which is to say, not strictly accurate) sense, a tensor is just a generalized term for a multi-dimensional matrix. In some deep learning frameworks, like PyTorch, a tensor is a specific type of data structure with properties and methods that support deep learning operations.*\n",
    "\n",
    "### Convolutional Layers\n",
    "Convolutional layers apply filters to a subregion of the input image, and *convolve* the filter across the image to extract features (such as edges, corners, etc.). For example, suppose the following matrix represents the pixels in a 6x6 image:\n",
    "\n",
    "$$\\begin{bmatrix}255 & 255 & 255 & 255 & 255 & 255\\\\255 & 255 & 0 & 0 & 255 & 255\\\\255 & 0 & 0 & 0 & 0 & 255\\\\255 & 0 & 0 & 0 & 0 & 255\\\\255 & 255 & 0 & 0 & 255 & 255\\\\255 & 255 & 255 & 255 & 255 & 255\\end{bmatrix}$$\n",
    "\n",
    "And let's suppose that a filter matrix is defined as a matrix of *weight* values like this:\n",
    "\n",
    "$$\\begin{bmatrix}0 & 1 & 0\\\\0 & 1 & 0\\\\0 & 1 & 0\\end{bmatrix}$$\n",
    "\n",
    "The convolution layer applies the filter to the image matrix one \"patch\" at a time; so the first operation would apply to the <span style=\"color:red\">red</span> elements below:\n",
    "\n",
    "$$\\begin{bmatrix}\\color{red}{255} & \\color{red}{255} & \\color{red}{255} & 255 & 255 & 255\\\\\\color{red}{255} & \\color{red}{255} & \\color{red}{0} & 0 & 255 & 255\\\\\\color{red}{255} & \\color{red}{0} & \\color{red}{0} & 0 & 0 & 255\\\\255 & 0 & 0 & 0 & 0 & 255\\\\255 & 255 & 0 & 0 & 255 & 255\\\\255 & 255 & 255 & 255 & 255 & 255\\end{bmatrix}$$\n",
    "\n",
    "To apply the filter, we multiply the patch area by the filter elementwise, and add the results:\n",
    "\n",
    "$$\\begin{bmatrix}255 & 255 & 255\\\\255 & 255 & 0\\\\255 & 0 & 0\\end{bmatrix} \\times \\begin{bmatrix}0 & 1 & 0\\\\0 & 1 & 0\\\\0 & 1 & 0\\end{bmatrix}= \\begin{bmatrix}(255 \\times 0) + (255 \\times 1) + (255 \\times 0) & +\\\\ (255 \\times 0) + (255 \\times 1) + (0 \\times 0) & + \\\\ (255 \\times 0) + (0 \\times 1) + (0 \\times 0)\\end{bmatrix}  = 510$$\n",
    "\n",
    "This result is then used as the value for the first element of a feature map:\n",
    "\n",
    "$$\\begin{bmatrix}\\color{red}{510} & ? & ? & ?\\\\? & ? & ? & ?\\\\? & ? & ? & ?\\\\? & ? & ? & ?\\end{bmatrix}$$\n",
    "\n",
    "Next we move the patch along one pixel and apply the filter to the new patch area:\n",
    "\n",
    "$$\\begin{bmatrix}255 & \\color{red}{255} & \\color{red}{255} & \\color{red}{255} & 255 & 255\\\\255 & \\color{red}{255} & \\color{red}{0} & \\color{red}{0} & 255 & 255\\\\255 & \\color{red}{0} & \\color{red}{0} & \\color{red}{0} & 0 & 255\\\\255 & 0 & 0 & 0 & 0 & 255\\\\255 & 255 & 0 & 0 & 255 & 255\\\\255 & 255 & 255 & 255 & 255 & 255\\end{bmatrix}$$\n",
    "\n",
    "$$\\begin{bmatrix}255 & 255 & 255\\\\255 & 0 & 0\\\\0 & 0 & 0\\end{bmatrix} \\times \\begin{bmatrix}0 & 1 & 0\\\\0 & 1 & 0\\\\0 & 1 & 0\\end{bmatrix}= \\begin{bmatrix}(255 \\times 0) + (255 \\times 1) + (255 \\times 0) & +\\\\ (255 \\times 0) + (0 \\times 1) + (0 \\times 0) & + \\\\ (0 \\times 0) + (0 \\times 1) + (0 \\times 0)\\end{bmatrix}  = 255 $$\n",
    "\n",
    "So can fill in that value on our feature map:\n",
    "$$\\begin{bmatrix}510 & \\color{red}{255} & ? & ?\\\\? & ? & ? & ?\\\\? & ? & ? & ?\\\\? & ? & ? & ?\\end{bmatrix}$$\n",
    "\n",
    "Then we just repeat the process, moving the patch across the entire image matrix until we have a completed feature map like this:\n",
    "\n",
    "$$\\begin{bmatrix}510 & 255 & 255 & 510\\\\255 & 0 & 0 & 255\\\\255 & 0 & 0 & 255\\\\510 & 255 & 255 & 510\\end{bmatrix}$$\n",
    "\n",
    "You'll have noticed that as a result of convolving a patch across the original image, we've \"lost\" a 1-pixel strip around the edge. Typically, we apply a *padding* rule to keep the convolved image the same size as the original image, often by simply filling a 1-pixel wide edge with 0 values, like this:\n",
    "\n",
    "$$\\begin{bmatrix}0 & 0 & 0 & 0 & 0 & 0\\\\0 & 510 & 255 & 255 & 510 & 0\\\\0 & 255 & 0 & 0 & 255 & 0\\\\0 & 255 & 0 & 0 & 255 & 0\\\\0 & 510 & 255 & 255 & 510 & 0\\\\0 & 0 & 0 & 0 & 0 & 0\\end{bmatrix}$$\n",
    "\n",
    "### Pooling Layers\n",
    "After using one or more convolution layers to create a filter map, you can use a pooling layer to  reduce the number of dimensions in the matrix. A common technique is to use *MaxPooling*, in which a patch is applied to the matrix and the maximum value within the mask is retained while the others are discarded.\n",
    "\n",
    "For example, we could apply a 2x2 patch to our feature map to extract the largest value in each 2x2 subarea:\n",
    "\n",
    "$$\\begin{bmatrix}\\color{blue}{0} & \\color{blue}{0} & \\color{green}{0} & \\color{green}{0} & \\color{red}{0} & \\color{red}{0}\\\\\\color{blue}{0} & \\color{blue}{510} & \\color{green}{255} & \\color{green}{255} & \\color{red}{510} & \\color{red}{0}\\\\\\color{magenta}{0} & \\color{magenta}{255} & \\color{orange}{0} & \\color{orange}{0} & \\color{cyan}{255} & \\color{cyan}{0}\\\\\\color{magenta}{0} & \\color{magenta}{255} & \\color{orange}{0} & \\color{orange}{0} & \\color{cyan}{255} & \\color{cyan}{0}\\\\\\color{brown}{0} & \\color{brown}{510} & 255 & 255 & \\color{yellow}{510} & \\color{yellow}{0}\\\\\\color{brown}{0} & \\color{brown}{0} & 0 & 0 & \\color{yellow}{0} & \\color{yellow}{0}\\end{bmatrix}\\Longrightarrow \\begin{bmatrix}\\color{blue}{510} & \\color{green}{255} & \\color{red}{510}\\\\\\color{magenta}{255} & \\color{orange}{0} & \\color{cyan}{255}\\\\\\color{brown}{510} & 255 & \\color{yellow}{510}\\end{bmatrix}$$\n",
    "\n",
    "### Activation Functions\n",
    "After each layer of filtering or pooling, it's common to apply a *rectified linear unit (ReLU)* function to the feature maps that have been produced. This has the effect of ensuring that all values in the feature maps are zero or higher.\n",
    "\n",
    "### Dropout Layers\n",
    "In any machine learning training process, there is a danger of *overfitting* the model to the training data. In other words, you might end with a model that works extremely well with the data on which it was trained, but can't generalize effectively to classify new images. One way in which you can reduce the risk of overfitting is to randomly drop some of the feature maps.\n",
    "\n",
    "### Dense (Fully-Connected) Layers\n",
    "After the previous layers have created feature maps, a final linear *fully-connected* layer is used to generate class predictions - you can think of the fully-connected layer as being the endpoint of the classifier what determines which combination of features found in the previous layers \"adds up\" to a particular class. To create a fully-connected layer, the feature maps are flattened into a single 1-dimensional matrix and a function is applied to calculate the probability for each class that the model is designed to predict - usually this final function is a *Sigmoid* or *SoftMax* function that assigns a value between 0 and 1 to each class, with the total of these assignments adding to 1:\n",
    "\n",
    "$$\\begin{bmatrix}510 & 255 & 510\\\\255 & 0 & 255\\\\510 & 255 & 510\\end{bmatrix}\\begin{bmatrix}255 & 255 & 510\\\\255 & 0 & 255\\\\510 & 255 & 255\\end{bmatrix}...$$\n",
    "\n",
    "$$ \\Downarrow $$\n",
    "\n",
    "$$\\begin{bmatrix}510 & 255 & 510 & 255 & 0 & 255 & 510 & 255 & 510 & 255 & 255 & 510 & 255 & 0 & 255 & 510 & 255 & 255 ...\\end{bmatrix}$$\n",
    "\n",
    "$$ \\Downarrow $$\n",
    "\n",
    "$$\\begin{bmatrix}C_{1} & C_{2} & C_{3} \\\\ 0.15 & 0.8 & 0.05\\end{bmatrix}$$\n",
    "\n",
    "### Backpropagation\n",
    "When we train a CNN, we perform mulitple passes forward through the network of layers, and then use a *loss function* to measure the difference between the output values (which you may recall are probability predictions for each class) and the actual values for the known image classes used to train the model (in other words, 1 for the correct class and 0 for all the others). For example, in the example above the predicted probabilities are 0.15 for C<sub>1</sub>, 0.8 for C<sub>2</sub>, and 0.05 for C<sub>3</sub>. Let's suppose that the image in question is an example of C<sub>2</sub>, so the expected output is actually 0 for C<sub>1</sub>, 1 for C<sub>2</sub>, and 0 for C<sub>3</sub>. The error (or *loss*) represents how far from the expected values our results are.\n",
    "\n",
    "Having calculated the loss, the training process uses a specified *optimizer* to calculate the derivitive of the loss function with respect to the weights and biases in the network layers, and determine how best to adjust them to reduce the loss. We then go backwards through the network, adjusting the weights before the next forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a CNN\n",
    "There are several commonly used frameworks for creating CNNs, including *PyTorch*, *Tensorflow*, the *Microsoft Cognitive Toolkit (CNTK)*, and *Keras* (which is a high-level API that can use Tensorflow or CNTK as a back end). \n",
    "\n",
    "### A Simple Example\n",
    "The example we'll use to explore this is a classification model that can classify images of geometric shapes.\n",
    "\n",
    "First, we'll generate some images for our classification model. Run the cell below to do that (note that it may take several minutes to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image folder already exists. Enter Y to replace it (this can take a while!). \n",
      "Y\n",
      "Deleting old images...\n",
      "Generating new images...\n",
      "Progress:0%\n",
      "Progress:6%\n",
      "Progress:12%\n",
      "Progress:19%\n",
      "Progress:25%\n",
      "Progress:31%\n",
      "Progress:38%\n",
      "Progress:44%\n",
      "Progress:50%\n",
      "Progress:56%\n",
      "Progress:62%\n",
      "Progress:69%\n",
      "Progress:75%\n",
      "Progress:81%\n",
      "Progress:88%\n",
      "Progress:94%\n",
      "Image files ready in shapes folder!\n"
     ]
    }
   ],
   "source": [
    "# Function to create a random image (of a square, circle, or triangle)\n",
    "def create_image (size, shape):\n",
    "    from random import randint\n",
    "    import numpy as np\n",
    "    from PIL import Image, ImageDraw\n",
    "    \n",
    "    xy1 = randint(10,40)\n",
    "    xy2 = randint(60,100)\n",
    "    col = (randint(0,200), randint(0,200), randint(0,200))\n",
    "\n",
    "    img = Image.new(\"RGB\", size, (255, 255, 255))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    if shape == 'circle':\n",
    "        draw.ellipse([(xy1,xy1), (xy2,xy2)], fill=col)\n",
    "    elif shape == 'square':\n",
    "        draw.rectangle([(xy1,xy1), (xy2,xy2)], fill=col)\n",
    "    else: # triangle\n",
    "        draw.polygon([(xy1,xy1), (xy2,xy2), (xy2,xy1)], fill=col)\n",
    "    del draw\n",
    "    \n",
    "    return np.array(img)\n",
    "\n",
    "# function to create a dataset of images\n",
    "def generate_image_data (classes, size, cases, img_dir):\n",
    "    import os, shutil\n",
    "    from PIL import Image\n",
    "    \n",
    "    if os.path.exists(img_dir):\n",
    "        replace_folder = input(\"Image folder already exists. Enter Y to replace it (this can take a while!). \\n\")\n",
    "        if replace_folder == \"Y\":\n",
    "            print(\"Deleting old images...\")\n",
    "            shutil.rmtree(img_dir)\n",
    "        else:\n",
    "            return # Quit - no need to replace existing images\n",
    "    os.makedirs(img_dir)\n",
    "    print(\"Generating new images...\")\n",
    "    i = 0\n",
    "    while(i < (cases - 1) / len(classes)):\n",
    "        if (i%25 == 0):\n",
    "            print(\"Progress:{:.0%}\".format((i*len(classes))/cases))\n",
    "        i += 1\n",
    "        for classname in classes:\n",
    "            img = Image.fromarray(create_image(size, classname))\n",
    "            saveFolder = os.path.join(img_dir,classname)\n",
    "            if not os.path.exists(saveFolder):\n",
    "                os.makedirs(saveFolder)\n",
    "            imgFileName = os.path.join(saveFolder, classname + str(i) + '.jpg')\n",
    "            try:\n",
    "                img.save(imgFileName)\n",
    "            except:\n",
    "                try:\n",
    "                    # Retry (resource constraints in Azure notebooks can cause occassional disk access errors)\n",
    "                    img.save(imgFileName)\n",
    "                except:\n",
    "                    # We gave it a shot - time to move on with our lives\n",
    "                    print(\"Error saving image\", imgFileName)\n",
    "            \n",
    "# Our classes will be circles, squares, and triangles\n",
    "classnames = ['circle', 'square', 'triangle']\n",
    "\n",
    "# All images will be 128x128 pixels\n",
    "img_size = (128,128)\n",
    "\n",
    "# We'll store the images in a folder named 'shapes'\n",
    "folder_name = 'shapes'\n",
    "\n",
    "# Generate 1200 random images.\n",
    "generate_image_data(classnames, img_size, 1200, folder_name)\n",
    "\n",
    "print(\"Image files ready in %s folder!\" % folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a CNN with PyTorch\n",
    "\n",
    "First, let's import the PyTorch libraries we'll need.\n",
    "\n",
    "> *Note: The following `pip install` commands install the latest version of PyTorch on Linux, which is appropriate for the Azure DSVM environment. For instructions on how to install the PyTorch and TorchVision packages on your own system, see https://pytorch.org/get-started/locally/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: torch in /data/anaconda/envs/py35/lib/python3.5/site-packages\n",
      "Requirement already up-to-date: numpy in /data/anaconda/envs/py35/lib/python3.5/site-packages (from torch)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already up-to-date: torchvision in /data/anaconda/envs/py35/lib/python3.5/site-packages\n",
      "Requirement already up-to-date: six in /data/anaconda/envs/py35/lib/python3.5/site-packages (from torchvision)\n",
      "Requirement already up-to-date: torch>=1.1.0 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from torchvision)\n",
      "Requirement already up-to-date: numpy in /data/anaconda/envs/py35/lib/python3.5/site-packages (from torchvision)\n",
      "Requirement already up-to-date: pillow>=4.1.1 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from torchvision)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Libraries imported - ready to use PyTorch 1.1.0\n"
     ]
    }
   ],
   "source": [
    "# Install PyTorch\n",
    "import sys\n",
    "! {sys.executable} -m pip install --upgrade torch\n",
    "! {sys.executable} -m pip install --upgrade torchvision\n",
    "\n",
    "# Import PyTorch libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Other libraries we'll use\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported - ready to use PyTorch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "PyTorch includes functions for loading and transforming data. We'll use these to create an iterative loader for training data, and a second iterative loader for test data (which we'll use to validate the trained model). The loaders will randomly flip some of the images horizontally to introduce more variation into the training data and help avoid overfitting. See https://pytorch.org/docs/stable/torchvision/transforms.html#transforms-on-pil-image for more details about image transforms that you can use to perform this kind of *augmentation*. Then the loader transforms the image data into *tensors*, which are the core data structure used in PyTorch, and normalizes them so that the pixel values are in a scale with a mean of 0.5 and a standard deviation of 0.5.\n",
    "\n",
    "Run the following cell to define the data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 classes:\n",
      "['circle', 'square', 'triangle']\n"
     ]
    }
   ],
   "source": [
    "# Function to ingest data using training and test loaders\n",
    "def load_dataset(data_path):\n",
    "    # Load all of the images\n",
    "    transformation = transforms.Compose([\n",
    "        # Randomly flip images\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        # transform to tensors\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize the pixel values (in R, G, and B channels)\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # Load all of the images, transforming them\n",
    "    full_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        transform=transformation\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Split into training (70% and testing (30%) datasets)\n",
    "    train_size = int(0.7 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "    \n",
    "    # define a loader for the training data we can iterate through in 50-image batches\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=30,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # define a loader for the testing data we can iterate through in 50-image batches\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=30,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "        \n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "# Now load the images from the shapes folder\n",
    "data_path = 'shapes/'\n",
    "\n",
    "# Get the class names\n",
    "classes = os.listdir(data_path)\n",
    "classes.sort()\n",
    "print(len(classes), 'classes:')\n",
    "print(classes)\n",
    "\n",
    "# Get the iterative dataloaders for test and training data\n",
    "train_loader, test_loader = load_dataset(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Neural Network Model\n",
    "In PyTorch, you define a neural network model as a class that is derived from the **nn.Module** base class. Your class must define the layers in your network, and provide a **forward** method that is used to process data through the layers of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (drop): Dropout2d(p=0.2)\n",
      "  (fc): Linear(in_features=24576, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create a neural net class\n",
    "class Net(nn.Module):\n",
    "    # Constructor\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Our images are RGB, so input channels = 3. We'll apply 12 filters in the first convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # A second convolutional layer takes 12 input channels, and generates 24 outputs\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # We'll apply max pooling with a kernel size of 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "                \n",
    "        # A drop layer deletes 20% of the features to help prevent overfitting\n",
    "        self.drop = nn.Dropout2d(p=0.2)\n",
    "        \n",
    "        # Our 128x128 image tensors will be pooled twice with a kernel size of 2. 128/2/2 is 32.\n",
    "        # So our feature tensors are now 32 x 32, and we've generated 24 of them\n",
    "        # We need to flatten these and feed them to a fully-connected layer\n",
    "        # to map them to  the probability for each class\n",
    "        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use a relu activation function after layer 1 (convolution 1 and pool)\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "      \n",
    "        # Use a relu activation function after layer 2 (convolution 2 and pool)\n",
    "        x = F.relu(self.pool(self.conv2(x)))\n",
    "        \n",
    "        # Select some features to drop after the 2nd convolution to prevent overfitting\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        # Only drop the features if this is a training pass\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 32 * 32 * 24)\n",
    "        # Feed to fully-connected layer to predict class\n",
    "        x = self.fc(x)\n",
    "        # Return class probabilities via a softmax function \n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "device = \"cpu\"\n",
    "if (torch.cuda.is_available()):\n",
    "    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n",
    "    device = \"cuda\"\n",
    "\n",
    "# Create an instance of the model class and allocate it to the device\n",
    "model = Net(num_classes=len(classes)).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "Now that we've defined a class for the network, we can train it using the image data.\n",
    "\n",
    "Training consists of an iterative series of forward passes in which the training data is processed in batches by the layers in the network, and the optimizer goes back and adjusts the weights. We'll also use a separate set of test images to test the model at the end of each iteration (or *epoch*) so we can track the performance improvement as the training process progresses. In this example, we'll use an optimizer based on the *Adam* algorithm and set its *learning rate* parameter (which determines how much the weights are adjusted after backpropagation identifies their affect on loss). These settings can have a significant impact on how well your model (and how quickly) your model learns the optimal weights and bias values required to predict accurately. \n",
    "\n",
    "> **Note**: For information about the optimizers available in PyTorch, see https://pytorch.org/docs/stable/optim.html#algorithms\n",
    "\n",
    "In the example below, we use 5 iterations (*epochs*) to train the model in 30-image batches, holding back 30% of the data for validation. After each epoch, the loss function measures the error (*loss*) in the model and adjusts the weights (which were randomly generated for the first iteration) to try to improve accuracy. \n",
    "\n",
    "> **Note**: We're only using 5 epochs to reduce the training time for this simple example. A real-world CNN is usually trained over more epochs than this. CNN model training is processor-intensive, so it's recommended to perform this on a system that can leverage GPUs (such as the Data Science Virtual Machine in Azure) to reduce training time. Status will be displayed as the training progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 1.104309\n",
      "\tTraining batch 2 Loss: 8.442365\n",
      "\tTraining batch 3 Loss: 2.127239\n",
      "\tTraining batch 4 Loss: 0.974276\n",
      "\tTraining batch 5 Loss: 1.092162\n",
      "\tTraining batch 6 Loss: 1.076040\n",
      "\tTraining batch 7 Loss: 1.203883\n",
      "\tTraining batch 8 Loss: 1.050526\n",
      "\tTraining batch 9 Loss: 1.067482\n",
      "\tTraining batch 10 Loss: 1.027783\n",
      "\tTraining batch 11 Loss: 1.115143\n",
      "\tTraining batch 12 Loss: 0.974363\n",
      "\tTraining batch 13 Loss: 0.985023\n",
      "\tTraining batch 14 Loss: 0.908297\n",
      "\tTraining batch 15 Loss: 0.933741\n",
      "\tTraining batch 16 Loss: 0.836267\n",
      "\tTraining batch 17 Loss: 0.868552\n",
      "\tTraining batch 18 Loss: 0.843448\n",
      "\tTraining batch 19 Loss: 0.764944\n",
      "\tTraining batch 20 Loss: 0.796142\n",
      "\tTraining batch 21 Loss: 0.691567\n",
      "\tTraining batch 22 Loss: 0.573300\n",
      "\tTraining batch 23 Loss: 0.730970\n",
      "\tTraining batch 24 Loss: 0.884020\n",
      "\tTraining batch 25 Loss: 1.065168\n",
      "\tTraining batch 26 Loss: 0.958997\n",
      "\tTraining batch 27 Loss: 0.760528\n",
      "\tTraining batch 28 Loss: 0.564573\n",
      "Training set: Average loss: 1.229325\n",
      "Validation set: Average loss: 0.592809, Accuracy: 254/360 (71%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.806549\n",
      "\tTraining batch 2 Loss: 0.553318\n",
      "\tTraining batch 3 Loss: 0.811731\n",
      "\tTraining batch 4 Loss: 0.941005\n",
      "\tTraining batch 5 Loss: 0.684166\n",
      "\tTraining batch 6 Loss: 0.789251\n",
      "\tTraining batch 7 Loss: 0.745746\n",
      "\tTraining batch 8 Loss: 0.802214\n",
      "\tTraining batch 9 Loss: 0.750576\n",
      "\tTraining batch 10 Loss: 0.739570\n",
      "\tTraining batch 11 Loss: 0.598144\n",
      "\tTraining batch 12 Loss: 0.734534\n",
      "\tTraining batch 13 Loss: 0.358238\n",
      "\tTraining batch 14 Loss: 0.474032\n",
      "\tTraining batch 15 Loss: 0.783310\n",
      "\tTraining batch 16 Loss: 0.669528\n",
      "\tTraining batch 17 Loss: 0.518002\n",
      "\tTraining batch 18 Loss: 0.636371\n",
      "\tTraining batch 19 Loss: 0.315082\n",
      "\tTraining batch 20 Loss: 0.659926\n",
      "\tTraining batch 21 Loss: 0.433243\n",
      "\tTraining batch 22 Loss: 0.536383\n",
      "\tTraining batch 23 Loss: 0.619239\n",
      "\tTraining batch 24 Loss: 0.968993\n",
      "\tTraining batch 25 Loss: 0.479323\n",
      "\tTraining batch 26 Loss: 0.601951\n",
      "\tTraining batch 27 Loss: 0.613256\n",
      "\tTraining batch 28 Loss: 0.539890\n",
      "Training set: Average loss: 0.648699\n",
      "Validation set: Average loss: 0.381657, Accuracy: 296/360 (82%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.387975\n",
      "\tTraining batch 2 Loss: 0.492063\n",
      "\tTraining batch 3 Loss: 0.567842\n",
      "\tTraining batch 4 Loss: 0.995291\n",
      "\tTraining batch 5 Loss: 0.403575\n",
      "\tTraining batch 6 Loss: 0.504703\n",
      "\tTraining batch 7 Loss: 0.776811\n",
      "\tTraining batch 8 Loss: 0.406907\n",
      "\tTraining batch 9 Loss: 0.519089\n",
      "\tTraining batch 10 Loss: 0.746679\n",
      "\tTraining batch 11 Loss: 0.648721\n",
      "\tTraining batch 12 Loss: 0.606588\n",
      "\tTraining batch 13 Loss: 0.490385\n",
      "\tTraining batch 14 Loss: 0.523073\n",
      "\tTraining batch 15 Loss: 0.481668\n",
      "\tTraining batch 16 Loss: 0.840732\n",
      "\tTraining batch 17 Loss: 0.390272\n",
      "\tTraining batch 18 Loss: 0.758264\n",
      "\tTraining batch 19 Loss: 0.428594\n",
      "\tTraining batch 20 Loss: 0.567868\n",
      "\tTraining batch 21 Loss: 0.537407\n",
      "\tTraining batch 22 Loss: 0.450081\n",
      "\tTraining batch 23 Loss: 0.477223\n",
      "\tTraining batch 24 Loss: 0.586156\n",
      "\tTraining batch 25 Loss: 0.549055\n",
      "\tTraining batch 26 Loss: 0.436552\n",
      "\tTraining batch 27 Loss: 0.246328\n",
      "\tTraining batch 28 Loss: 0.384671\n",
      "Training set: Average loss: 0.543020\n",
      "Validation set: Average loss: 0.266091, Accuracy: 317/360 (88%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.464179\n",
      "\tTraining batch 2 Loss: 0.247637\n",
      "\tTraining batch 3 Loss: 0.670304\n",
      "\tTraining batch 4 Loss: 0.505619\n",
      "\tTraining batch 5 Loss: 0.576069\n",
      "\tTraining batch 6 Loss: 0.712138\n",
      "\tTraining batch 7 Loss: 0.426224\n",
      "\tTraining batch 8 Loss: 0.537200\n",
      "\tTraining batch 9 Loss: 0.503237\n",
      "\tTraining batch 10 Loss: 0.553517\n",
      "\tTraining batch 11 Loss: 0.346332\n",
      "\tTraining batch 12 Loss: 0.612178\n",
      "\tTraining batch 13 Loss: 0.402367\n",
      "\tTraining batch 14 Loss: 0.326077\n",
      "\tTraining batch 15 Loss: 0.377907\n",
      "\tTraining batch 16 Loss: 0.490558\n",
      "\tTraining batch 17 Loss: 0.357805\n",
      "\tTraining batch 18 Loss: 0.600117\n",
      "\tTraining batch 19 Loss: 0.376204\n",
      "\tTraining batch 20 Loss: 0.461719\n",
      "\tTraining batch 21 Loss: 0.449091\n",
      "\tTraining batch 22 Loss: 0.417957\n",
      "\tTraining batch 23 Loss: 0.349829\n",
      "\tTraining batch 24 Loss: 0.602592\n",
      "\tTraining batch 25 Loss: 0.632941\n",
      "\tTraining batch 26 Loss: 0.345441\n",
      "\tTraining batch 27 Loss: 0.350437\n",
      "\tTraining batch 28 Loss: 0.346111\n",
      "Training set: Average loss: 0.465778\n",
      "Validation set: Average loss: 0.264708, Accuracy: 325/360 (90%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.319750\n",
      "\tTraining batch 2 Loss: 0.237872\n",
      "\tTraining batch 3 Loss: 0.293834\n",
      "\tTraining batch 4 Loss: 0.519431\n",
      "\tTraining batch 5 Loss: 0.384935\n",
      "\tTraining batch 6 Loss: 0.362156\n",
      "\tTraining batch 7 Loss: 0.532944\n",
      "\tTraining batch 8 Loss: 0.707272\n",
      "\tTraining batch 9 Loss: 0.508909\n",
      "\tTraining batch 10 Loss: 0.767469\n",
      "\tTraining batch 11 Loss: 0.451326\n",
      "\tTraining batch 12 Loss: 0.580948\n",
      "\tTraining batch 13 Loss: 0.487683\n",
      "\tTraining batch 14 Loss: 0.660745\n",
      "\tTraining batch 15 Loss: 0.632037\n",
      "\tTraining batch 16 Loss: 0.680734\n",
      "\tTraining batch 17 Loss: 0.639448\n",
      "\tTraining batch 18 Loss: 0.673139\n",
      "\tTraining batch 19 Loss: 0.815166\n",
      "\tTraining batch 20 Loss: 0.728637\n",
      "\tTraining batch 21 Loss: 0.645266\n",
      "\tTraining batch 22 Loss: 0.655997\n",
      "\tTraining batch 23 Loss: 0.749489\n",
      "\tTraining batch 24 Loss: 0.796660\n",
      "\tTraining batch 25 Loss: 0.684746\n",
      "\tTraining batch 26 Loss: 0.699483\n",
      "\tTraining batch 27 Loss: 0.671108\n",
      "\tTraining batch 28 Loss: 0.550976\n",
      "Training set: Average loss: 0.587077\n",
      "Validation set: Average loss: 0.539358, Accuracy: 270/360 (75%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    print(\"Epoch:\", epoch)\n",
    "    # Process the images in batches\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Use the CPU or GPU as appropriate\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Reset the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Push the data forward through the model layers\n",
    "        output = model(data)\n",
    "        \n",
    "        # Get the loss\n",
    "        loss = loss_criteria(output, target)\n",
    "\n",
    "        # Keep a running total\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print metrics so we see some progress\n",
    "        print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n",
    "            \n",
    "    # return average loss for the epoch\n",
    "    avg_loss = train_loss / (batch_idx+1)\n",
    "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
    "    return avg_loss\n",
    "            \n",
    "            \n",
    "def test(model, device, test_loader):\n",
    "    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        batch_count = 0\n",
    "        for data, target in test_loader:\n",
    "            batch_count += 1\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Get the predicted classes for this batch\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate the loss for this batch\n",
    "            test_loss += loss_criteria(output, target).item()\n",
    "            \n",
    "            # Calculate the accuracy for this batch\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += torch.sum(target==predicted).item()\n",
    "\n",
    "    # Calculate the average loss and total accuracy for this epoch\n",
    "    avg_loss = test_loss / batch_count\n",
    "    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    # return average loss for the epoch\n",
    "    return avg_loss\n",
    "    \n",
    "    \n",
    "# Use an \"Adam\" optimizer to adjust weights\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Specify the loss criteria\n",
    "loss_criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "# Track metrics in these arrays\n",
    "epoch_nums = []\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "# Train over 5 epochs (in a real scenario, you'd likely use many more)\n",
    "epochs = 5\n",
    "print('Training on', device)\n",
    "for epoch in range(1, epochs + 1):\n",
    "        train_loss = train(model, device, train_loader, optimizer, epoch)\n",
    "        test_loss = test(model, device, test_loader)\n",
    "        epoch_nums.append(epoch)\n",
    "        training_loss.append(train_loss)\n",
    "        validation_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the Loss History\n",
    "We tracked average training and validation loss for each epoch. We can plot these to verify that loss reduced as the model was trained, and to detect *over-fitting* (which is indicated by a continued drop in training loss after validation loss has levelled out or started to increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(epoch_nums, training_loss)\n",
    "plt.plot(epoch_nums, validation_loss)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['training', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model Performance\n",
    "We can see the final accuracy based on the test data, but typically we'll want to explore performance metrics in a little more depth. Let's plot a confusion matrix to see how well the model is predicting each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions from test set...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pytorch doesn't have a built-in confusion matrix metric, so we'll use SciKit-Learn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Set the model to evaluate mode\n",
    "model.eval()\n",
    "\n",
    "# Get predictions for the test data and convert to numpy arrays for use with SciKit-Learn\n",
    "print(\"Getting predictions from test set...\")\n",
    "truelabels = []\n",
    "predictions = []\n",
    "for data, target in test_loader:\n",
    "    for label in target.cpu().data.numpy():\n",
    "        truelabels.append(label)\n",
    "    for prediction in model.cpu()(data).data.numpy().argmax(1):\n",
    "        predictions.append(prediction)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(truelabels, predictions)\n",
    "plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "plt.xlabel(\"Predicted Shape\")\n",
    "plt.ylabel(\"True Shape\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Model\n",
    "Now that we have trained the model, we can save its weights. Then later, we can reload those weights into an instance of the same network and use it to predict classes from new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "# Save the model weights\n",
    "model_file = 'shape-classifier.pth'\n",
    "torch.save(model.state_dict(), model_file)\n",
    "print(\"Model saved.\")\n",
    "\n",
    "# Delete the existing model variable\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Model with New Data\n",
    "Now that we've trained and evaluated our model, we can use it to predict classes for new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triangle\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to predict the class of an image\n",
    "def predict_image(classifier, image_array):\n",
    "   \n",
    "    # Set the classifer model to evaluation mode\n",
    "    classifier.eval()\n",
    "    \n",
    "    # These are the classes our model can predict\n",
    "    class_names = ['circle', 'square', 'triangle']\n",
    "    \n",
    "    # Apply the same transformations as we did for the training images\n",
    "    transformation = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # Preprocess the imagees\n",
    "    image_tensor = torch.stack([transformation(image).float() for image in image_array])\n",
    "\n",
    "    # Turn the input into a Variable\n",
    "    input_features = Variable(image_tensor)\n",
    "\n",
    "    # Predict the class of each input image\n",
    "    predictions = classifier(input_features)\n",
    "    \n",
    "    predicted_classes = []\n",
    "    # Convert the predictions to a numpy array \n",
    "    for prediction in predictions.data.numpy():\n",
    "        # The prediction for each image is the probability for each class, e.g. [0.8, 0.1, 0.2]\n",
    "        # So get the index of the highest probability\n",
    "        class_idx = np.argmax(prediction)\n",
    "        # And append the corresponding class name to the results\n",
    "        predicted_classes.append(class_names[class_idx])\n",
    "    return np.array(predicted_classes)\n",
    "\n",
    "\n",
    "# Now let's try it with a new image\n",
    "from random import randint\n",
    "\n",
    "# Create a new model instance and load the weights\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load(model_file))\n",
    "\n",
    "# Create a random test image\n",
    "img = create_image ((128,128), classes[randint(0, len(classes)-1)])\n",
    "plt.imshow(img)\n",
    "\n",
    "# Create an array of (1) images to match the expected input format\n",
    "image_array = img.reshape(1, img.shape[0], img.shape[1], img.shape[2]).astype('float32')\n",
    "\n",
    "predicted_classes = predict_image(model, image_array)\n",
    "print(predicted_classes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
