{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying a Model as a Web Service\n",
    "\n",
    "> **Important**: This notebook assumes that you have previously used the **02 - Image Classification (PyTorch).ipynb** notebook to save a PyTorch-based shape classification model.\n",
    "\n",
    "It's no good being able to create an accurate model if you can't deploy it for use in an application or service. In this notebook, we'll explore the *Azure Machine Learning Service* and the associated *Azure Machine Learning SDK*; which together enable you to train, deploy, and manage machine learning models at scale.\n",
    "\n",
    "To use Azure Machine Learning, you're going to need an Azure subscription. If you don't already have one, you can sign up for a free trial at https://azure.microsoft.com/Account/Free.\n",
    "\n",
    "*Note: Azure Machine Learning provides a whole range of functionality to help you through the entire lifecycle of model development, training, evaluation, deployment, and management. We're going to focus on using it to deploy a trained model; but you can use it to do much, much more!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Brief Introduction to Containers\n",
    "When you access a web site or a software service across the internet, you're probably dimly aware that somewhere, the code for the service is hosted on a *server*. We tend to think of servers as being physical computers, but in recent years there's been a growth in *virtualization* technologies so that a computer can be virtualized in software, and multiple *virtual machines* can be hosted on a single physical server.\n",
    "\n",
    "Virtual machines (VMs) are useful - in fact, the Azure Data Science Virtual Machine (DSVM) is a good example of a VM that enables you to provision a computer that contains the operating system (OS) and all the software applications you need to work with data and build machine learning models, and then you can delete the VM when you're finished with it so that you only pay for what you use - very cool!\n",
    "\n",
    "However, it seems wasteful to provision a complete virtual machine, including the full OS and applications, just to host a simple software service - especially if you need to support multiple services, each one consuming its own VM. *Containers* are an evolutionary step beyond VMs. They contain only the OS components that are required for the specific software service they need to host. This makes them very small compared to full VMs, which in turn means that they're portable, and quick to deploy and start up.\n",
    "\n",
    "Containers themselves are hosted in a container environment that provides all the common services and OS functionality they require. During development, this environment is often a locally installed system called *Docker*. When hosting a service in the cloud however, you can use container services such as *Azure Container Instances* (ACI), which is useful for lightweight hosting and testing of containerized services; or *Azure Kubernetes Services*, which provides a scalable and highly-available environment for managing clusters of containers, based on the industry standard *Kubernetes* container hosting platform.\n",
    "\n",
    "In the rest of this notebook, we'll examine how you can use Azure Machine Learning Services to prepare a container image for your machine learning model, and deploy your model as a containerized web service that can be consumed by other applications that connect to it over an HTTP REST endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a classification model locally\n",
    "Let's start by verifying that the classification model you created previously works locally. First install the libraries you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: torch in /data/anaconda/envs/py36/lib/python3.6/site-packages (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /data/anaconda/envs/py36/lib/python3.6/site-packages (from torch) (1.16.2)\n",
      "Collecting torchvision\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/45/0f2f3062c92d9cf1d5d7eabd3cae88cea9affbd2b17fb1c043627838cb0a/torchvision-0.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.6MB)\n",
      "\u001b[K     |████████████████████████████████| 2.6MB 7.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /data/anaconda/envs/py36/lib/python3.6/site-packages (from torchvision) (1.16.2)\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.1.0 in /data/anaconda/envs/py36/lib/python3.6/site-packages (from torchvision) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /data/anaconda/envs/py36/lib/python3.6/site-packages (from torchvision) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /data/anaconda/envs/py36/lib/python3.6/site-packages (from torchvision) (6.0.0)\n",
      "Installing collected packages: torchvision\n",
      "  Found existing installation: torchvision 0.2.2\n",
      "    Uninstalling torchvision-0.2.2:\n",
      "\u001b[31mERROR: Could not install packages due to an EnvironmentError: [Errno 13] Permission denied: '__init__.py'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\u001b[0m\n",
      "Libraries imported - ready to use PyTorch 1.1.0\n"
     ]
    }
   ],
   "source": [
    "# Install PyTorch\n",
    "import sys\n",
    "! {sys.executable} -m pip install --upgrade torch\n",
    "! {sys.executable} -m pip install --upgrade torchvision\n",
    "\n",
    "# Import PyTorch libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"Libraries imported - ready to use PyTorch\", torch.__version__)\n",
    "\n",
    "# Other libraries we'll use\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll create a function to generate new images, and a function to classify an image using a specified classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a random image (of a square, circle, or triangle)\n",
    "def create_image (size, shape):\n",
    "    from random import randint\n",
    "    import numpy as np\n",
    "    from PIL import Image, ImageDraw\n",
    "    \n",
    "    xy1 = randint(10,40)\n",
    "    xy2 = randint(60,100)\n",
    "    col = (randint(0,200), randint(0,200), randint(0,200))\n",
    "\n",
    "    img = Image.new(\"RGB\", size, (255, 255, 255))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    if shape == 'circle':\n",
    "        draw.ellipse([(xy1,xy1), (xy2,xy2)], fill=col)\n",
    "    elif shape == 'square':\n",
    "        draw.rectangle([(xy1,xy1), (xy2,xy2)], fill=col)\n",
    "    else: # triangle\n",
    "        draw.polygon([(xy1,xy1), (xy2,xy2), (xy2,xy1)], fill=col)\n",
    "    del draw\n",
    "    \n",
    "    return np.array(img)\n",
    "\n",
    "# Function to predict the class of an image\n",
    "def predict_image(classifier, image_array):\n",
    "    import torch.utils.data as utils\n",
    "    import numpy as np\n",
    "    \n",
    "    # Set the classifer model to evaluation mode\n",
    "    classifier.eval()\n",
    "    \n",
    "    # Apply the same transformations as we did for the training images\n",
    "    transformation = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # Preprocess the image\n",
    "    image_tensor = torch.stack([transformation(image).float() for image in image_array])\n",
    "\n",
    "    # Turn the input into a Variable\n",
    "    input_features = Variable(image_tensor)\n",
    "\n",
    "    # Predict the class of the image\n",
    "    predictions = classifier(input_features)\n",
    "    \n",
    "    classnames = ['circle', 'square', 'triangle']\n",
    "    \n",
    "    predicted_classes = []\n",
    "    for prediction in predictions.data.numpy():\n",
    "        class_idx = np.argmax(prediction)\n",
    "        predicted_classes.append(classnames[class_idx])\n",
    "    return np.array(predicted_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you're ready to try to classify new images using the model you created in the previous challenge. You'll need to define a class for this model, and load the weights you saved previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model created from saved weights\n"
     ]
    }
   ],
   "source": [
    "# Define the Net class as used for training so we can load the trained weights\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        self.drop = nn.Dropout2d(p=0.2)\n",
    "        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "        x = F.relu(self.pool(self.conv2(x)))\n",
    "        x = F.relu(self.drop(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = x.view(-1, 32 * 32 * 24)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Create a new model and load the weights\n",
    "model_file = 'shape-classifier.pth'\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load(model_file))\n",
    "print(\"New model created from saved weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use the model to predict the class of a new image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triangle\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now let's try it with a new image\n",
    "from random import randint\n",
    "\n",
    "# Create a random test image\n",
    "classnames = ['circle', 'square', 'triangle']\n",
    "img_size = (128,128)\n",
    "img = create_image (img_size, classnames[randint(0, len(classnames)-1)])\n",
    "plt.imshow(img)\n",
    "\n",
    "# Create an array of (1) images to match the expected input format\n",
    "image_array = img.reshape(1, img.shape[0], img.shape[1], img.shape[2]).astype('float32')\n",
    "\n",
    "predicted_classes = predict_image(model, image_array)\n",
    "print(predicted_classes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks as though we have a working model. Now we're ready to use Azure Machine Learning to deploy it as a web service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Azure Machine Learning workspace\n",
    "\n",
    "To use Azure Machine Learning, you'll need to create a workspace in your Azure subscription.\n",
    "\n",
    "Your Azure subscription is identified by a subscription ID. To find this:\n",
    "1. Sign into the Azure portal at https://portal.azure.com.\n",
    "2. On the menu tab on the left, click &#128273; **Subscriptions**.\n",
    "3. View the list of your subscriptions and copy the ID for the subscription you want to use.\n",
    "4. Paste the subscription ID into the code below, and then run the cell to set the variable - you will use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace YOUR_SUBSCRIPTION_ID in the following variable assignment:\n",
    "SUBSCRIPTION_ID = 'YOUR_SUBSCRIPTION_ID'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy the model file as a web service, we'll use the Azure Machine Learning SDK.\n",
    "\n",
    "> Note: the Azure Machine Learning SDK is installed by default in Azure Notebooks and the Azure Data Science Virtual Machine, but you may want to ensure that it's upgraded to the latest version. If you're using your own Python environment, you'll need to install it using the instructions in the [Azure Machine Learning documentation](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-create-workspace-with-python)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "! {sys.executable} -m pip install azureml-sdk --upgrade\n",
    "\n",
    "import azureml.core\n",
    "print(azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manage the deployment, we need an Azure ML workspace. Create one in your Azure subscription by running the following cell. If you're signed into Azure notebooks using the same credentials as your Azure subscription, you may be prompted to grant this notebooks project permission to use your Azure credentials. Otherwise, you'll be prompted to authenticate by entering a code at a given URL, so just click the link that's displayed and enter the specified code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "ws = Workspace.create(name='aml_workspace', # or another name of your choosing\n",
    "                      subscription_id=SUBSCRIPTION_ID,\n",
    "                      resource_group='aml_resource_group', # or another name of your choosing\n",
    "                      create_resource_group=True,\n",
    "                      location='eastus2' # or other supported Azure region\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a workspace, you can save the configuration so you can reconnect to it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "# Save the workspace config\n",
    "ws.write_config()\n",
    "\n",
    "# Reconnect to the workspace (if you're not already signed in, you'll be prompted to authenticate with a code as before)\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the model in Azure ML\n",
    "\n",
    "You've created a model and saved it locally. Now you can register this model in your Azure ML workspace, which will enable you to manage and deploy it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model = Model.register(model_path = \"shape-classifier.pth\",\n",
    "                       model_name = \"shape-classifier-pytorch\",\n",
    "                       tags = {'area': \"shapes\", 'type': \"classifier\"},\n",
    "                       description = \"PyTorch shape classifier\",\n",
    "                       workspace = ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a *scoring* file\n",
    "Your web service will need some Python code to load the input data, get the model, and generate and return a prediction. We'll save this code in a *scoring* file that will be deployed to the web service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score_pytorch.py\n",
    "\n",
    "# create a scoring script that loads and infers from the model\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from azureml.core.model import Model\n",
    "\n",
    "def init():\n",
    "    try:\n",
    "        global model\n",
    "        MODEL_NAME = 'shape-classifier-pytorch'\n",
    "        # retieve the local path to the model using the model name\n",
    "        MODEL_PATH = Model.get_model_path(MODEL_NAME)\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        model = Net()\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "        return json.dumps({\"error\": result})\n",
    "\n",
    "# REST API served by Azure ML supports json input\n",
    "def run(json_data):\n",
    "    try:\n",
    "        data = np.array(json.loads(json_data)['data']).astype('float32')\n",
    "        predictions = predict_image(model, data)\n",
    "        return json.dumps(predictions.tolist())\n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "        return json.dumps({\"error\": result})\n",
    "\n",
    "# Function to predict\n",
    "def predict_image(classifier, image_array):\n",
    "    import torch\n",
    "    import torch.utils.data as utils\n",
    "    from torchvision import transforms\n",
    "    from torch.autograd import Variable\n",
    "    import numpy\n",
    "    \n",
    "    classifier.eval()\n",
    "    \n",
    "    transformation = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    image_tensor = torch.stack([transformation(image).float() for image in image_array])\n",
    "\n",
    "    input_features = Variable(image_tensor)\n",
    "    predictions = classifier(input_features)\n",
    "    \n",
    "    classnames = ['circle', 'square', 'triangle']\n",
    "    \n",
    "    predicted_classes = []\n",
    "    for prediction in predictions.data.numpy():\n",
    "        class_idx = np.argmax(prediction)\n",
    "        predicted_classes.append(classnames[class_idx])\n",
    "    return np.array(predicted_classes)\n",
    "    \n",
    "# Define the Net class as used for training so we can load the trained weights\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        self.drop = nn.Dropout2d(p=0.2)\n",
    "        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "        x = F.relu(self.pool(self.conv2(x)))\n",
    "        x = F.relu(self.drop(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = x.view(-1, 32 * 32 * 24)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an *environment* file\n",
    "The web service will be hosted in a container, and the container will need to install any Python dependencies when it gets initialized. In this case, our scoring code requires the **torch** and **torchvision** Python libraries, so we'll create a .yml file that tells the container host to install these into the environment along with the default libraries used by Azure ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "myenv = CondaDependencies()\n",
    "myenv.add_conda_package(\"pytorch\")\n",
    "myenv.add_conda_package(\"torchvision\")\n",
    "myenv.add_channel(\"pytorch\")\n",
    "\n",
    "env_file = \"env_pytorch.yml\"\n",
    "\n",
    "with open(env_file,\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())\n",
    "print(\"Saved dependency info in\", env_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the web service \n",
    "Now we're ready to deploy. We'll deploy the container a service named **pytorch-shape-classifier**.\n",
    "The deployment process includes the following steps:\n",
    "1. Define an *inference configuration*, which includes the scoring and environment files required to load and use the model.\n",
    "2. Define a *deployment configuration* that defines the execution environment in which the service will be hosted. In this case, an Azure Container Instance.\n",
    "3. Deploy the web service.\n",
    "4. Verify the status of the deployed service.\n",
    "\n",
    "This will take some time. When deployment has completed successfully, you'll see a status of **Healthy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "inference_config = InferenceConfig(runtime= \"python\", \n",
    "                                   entry_script=\"score_pytorch.py\",\n",
    "                                   conda_file=\"env_pytorch.yml\")\n",
    "\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1)\n",
    "\n",
    "service_name = \"pytorch-shape-classifier\"\n",
    "\n",
    "service = Model.deploy(ws, service_name, [model], inference_config, deployment_config)\n",
    "\n",
    "service.wait_for_deployment(True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the web service\n",
    "With the service deployed, now we can test it by using it to predict the shape of a new image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from random import randint\n",
    "\n",
    "# Create a random test image\n",
    "img = create_image (img_size, classnames[randint(0, len(classnames)-1)])\n",
    "plt.imshow(img)\n",
    "\n",
    "# Modify the image data to create an array of 1 image, matching the format of the training features\n",
    "input_array = img.reshape(1, img.shape[0], img.shape[1], img.shape[2])\n",
    "\n",
    "# Convert the array to JSON format\n",
    "input_json = json.dumps({\"data\": input_array.tolist()})\n",
    "\n",
    "# Call the web service, passing the input data (the web service will also accept the data in binary format)\n",
    "predictions = service.run(input_data = input_json)\n",
    "\n",
    "# Get the predicted class - it'll be the first (and only) one.\n",
    "classname = json.loads(predictions)[0]\n",
    "print('The image is a', classname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also send a batch of images to the service, and get back a prediction for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Create three random test images\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "images = []\n",
    "i = 0\n",
    "while(i < 3):  \n",
    "    # Create a new image\n",
    "    img = create_image(img_size, classnames[randint(0, len(classnames)-1)])\n",
    "    # Plot the image\n",
    "    a=fig.add_subplot(1,3,i + 1)\n",
    "    imgplot = plt.imshow(img)\n",
    "    # Add the image to an array to be submitted as a batch\n",
    "    images.append(img.tolist())\n",
    "    i += 1\n",
    "\n",
    "# Convert the array to JSON format\n",
    "input_json = json.dumps({\"data\": images})\n",
    "\n",
    "# Call the web service, passing the input data\n",
    "predictions = service.run(input_data = input_json)\n",
    "\n",
    "# Get the predicted classes\n",
    "print(json.loads(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Web Service from Other Applications\n",
    "The code above uses the Azure ML SDK to connect to the containerized web service and use it to generate predictions from your image classification model. In production, the model is likely to be consumed by business applications that make HTTP requests to the web service.\n",
    "\n",
    "Let's determine the URL to which these applications must submit their requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = service.scoring_uri\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the endpoint URI, an application can simply make an HTTP request, sending the image data in JSON (or binary) format, and receive back the predicted class(es)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import requests\n",
    "import json\n",
    "from random import randint\n",
    "\n",
    "# Create a random test image\n",
    "img = create_image (img_size, classnames[randint(0, len(classnames)-1)])\n",
    "plt.imshow(img)\n",
    "\n",
    "# Create an array of (1) images to match the expected input format\n",
    "image_array = img.reshape(1, img.shape[0], img.shape[1], img.shape[2])\n",
    "\n",
    "# Convert the array to a serializable list in a JSON document\n",
    "input_json = json.dumps({\"data\": image_array.tolist()})\n",
    "\n",
    "# Set the content type\n",
    "headers = { 'Content-Type':'application/json' }\n",
    "\n",
    "predictions = requests.post(endpoint, input_json, headers = headers)\n",
    "print(json.loads(predictions.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting the Service\n",
    "When we're finished with the service, we can delete it to avoid incurring charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()\n",
    "print(\"Service deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you're finished with the workspace, you can delete that too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg = ws.resource_group\n",
    "ws.delete()\n",
    "print(\"Workspace deleted. You should delete the '%s' resource group in your Azure subscription.\" % rg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn more\n",
    "Take a look at the Azure Machine Learning documentation at https://docs.microsoft.com/en-us/azure/machine-learning/service/."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
